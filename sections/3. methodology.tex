\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{images/}}}


\begin{document}

\section{Methodology}
\subsection{Watching conspiracy videos}
In order to determine how different watch strategies affect the YouTube algorithm, a python script was
created to automatically log into a Google account and proceed to watch YouTube videos. The script was
made using Selenium WebDriver: a suite of tools used for browser automation. 

\subsubsection{Google login}
Due to Google's strict policy regarding automation within their ecosystem, many obstacles are put into
place to prevent users from logging into a Google account using automated software such as a selenium 
script. To circumvent this restriction, two steps had to be taken. Firstly, the selenium WebDriver had to
be accompanied by the selenium-stealth package, which removes metadata about the current browser, so that
it is less obvious that a WebDriver is being used. Additionally, because this metadata was removed, the 
Google login service was unable to check what browser the client was using. This results in a warning to 
the user that their current browser may be insecure, which prohibits them from logging in. To avoid this 
warning, the Google account needs to have been created within a WebDriver, such as Google's ChromeDriver 
or Mozilla's GeckoDriver. Therefore, all twenty accounts were manually created in ChromeDriver. Since 
Google accounts require a phone number verification upon creation, six free (prepaid) SIM cards were 
ordered from various providers in order to create the accounts. Each SIM card could create two to three 
accounts before it was blocked due to being used too many times. 

\subsubsection{The watch strategies}
After all accounts had been created, they were subdivided into four distinct watch strategies, making for
a total of five accounts per strategy. The first watch strategy is the simplest one. The bots following 
it will watch random, non-conspiracy videos from a dataset. This watch strategy is used as the 
baseline to compare the other three strategies to. The second strategy is similar to the first: the 
adhering bots watch random conspiracy videos from a dataset. This strategy was not added to 
study the origination of filter bubbles, since it is unlikely that a filter bubble will come from this 
strategy. Considering the video choices will be all over the place, it will be difficult for the 
algorithm to determine the specific interest of the user. However, comparing how the algorithm 
responds to conspiracy content in general as opposed to regular content might still yield interesting 
results. The second-to-last strategy starts off in the same way: it chooses a random conspiracy video 
from a dataset to watch. However, after having watched the initial video, it will start watching the 
recommended videos displayed next to the current video. These recommendations consist of a combination of
recommendations based on the content of the current video and the personalized recommendations of the 
user. By using this strategy, bots are likely to go \textit{down the rabbit hole} and eventually end up 
in a filter bubble. Finally, the last strategy will be similar to the previous one, though with one 
alteration: rather than choosing a recommended video from the list of recommendations next to the current
video, it will choose a recommended video from the YouTube homepage of the account. Compared to the third
strategy, this will lead to the user watching more personalized recommendations rather than content-based
recommendations, possibly speeding up the creation and/or increasing the strength of the filter bubble. 
Each strategy will be executed by five different accounts in order to decrease the probability of a 
random streak of videos altering the result. The individual accounts will watch a total of fifteen
videos as described by their watch strategy, for a total of three hundred videos watched by the script. 

Additionally, to simulate real-world user behavior, the average watch time for the videos will be normally
distributed with a mean of 55\% and a standard deviation of 25\% \citep{park2016data}. The watch time will 
not be able to exceed a value of 100, as a video cannot be watched for more than 100\%. In the same vein, the
clicking behavior of users will be simulated as accurately as possible. The probability of a user clicking on
a video at position $k$ within a given list of recommendations (its click-through rate: $CTR$), will be 
determined using the following formula:

\begin{equation}
CTR(k; N, \alpha) = \frac{1/k^\alpha}{\sum_{n=1}^{N} (1/n^\alpha)}
\end{equation}

Wherein $N$ is the total number of recommendations and $\alpha$ is the distribution's exponent value ($\alpha
= 0.78$) \citep{zhou2010impact}. Using this formula, when considering the first twenty recommendations, the
first recommendation will have a click-through rate of approximately 20.6\%, after which the CTR quickly
decreases, until a probability of 1.9\% at the twentieth recommendation. 

\subsubsection{Running the bots}
After the accounts were logged in, they started watching YouTube videos according to their watch
strategy. However, some restrictions were put into place to make sure the bots did not take too long
(considering three hundred videos had to be watched in total, some limitations had to apply). For
example, the bots were not allowed to watch videos over an hour long, nor were they allowed to watch live
streams, as those could theoretically go on infinitely. Additionally, the random videos at the start of
the third and fourth strategy were first manually inspected to make sure the bots would not start the
experiment by watching a falsely flagged conspiracy video. Considering the way in which the dataset was
created, it is possible that some videos that are flagged as conspiracy videos are, in reality, normal 
videos. This could happen whenever a conspiracy channel uploads a regular video for once (e.g. a holiday 
video, promotion of some product, etc.). These \textit{false positives} are far and few between, however,
having one of them be selected as the first video for strategy three or four had to be prevented, as that
could greatly alter the final results. With these restrictions in mind, the following script was created
and run for all twenty bots, keeping track of the videos they watched and the homepage recommendations they
had after each video:

\vspace{0.25in}

\nolinenumbers

\begin{algorithm}[H]
 \KwData{User information and a video dataset}
 \KwResult{The watched videos and homepage recommendations of the user}
 
 \vspace{0.075in}
 initialize WebDriver\;
 log into Google account\;

 \For{twenty videos}{
  \eIf{there is a recommendation to be watched}{
    go to the link\;
   }{
    pick a random video to watch based on usertype\;
    determine how long it will get watched\;
    go to the link\;
   }
   \vspace{0.05in}
   get video metadata and store for overview of watched videos\;
   watch video for given amount of time\;
   \vspace{0.05in}
   \uIf{usertype == 3}{
    pick recommendation next to current video to watch next\;
    determine watch time for found recommendation\;
   }
   \vspace{0.05in}
   go to YouTube homepage\;
   store current recommendations for overview\;
   \vspace{0.05in}
   \uIf{usertype == 4}{
    pick homepage recommendation to watch next\;
    determine watch time for found recommendation\;
   }
 }
 \vspace{0.05in}
 \textbf{return} watched videos and homepage recommendations\;
 \caption{Watch YouTube videos according to a watch strategy}
\end{algorithm}

\vspace{0.25in}

\noindent Running the script for all twenty bots resulted in two different datasets: the first containing
the videos watched by the bots, including their view count, likes/dislikes, and upload date; and one 
containing the homepage recommendations for all bots, after each number of videos watched.
To determine the influence of the watch strategies on the algorithm, the recommendations were labeled as 
being either conspiracy or non-conspiracy videos by the classifier. By then grouping all recommendations 
by their watch strategy and the number of videos watched before them (i.e. the recommendations after the 
third video watched by all bots with strategy one), it was possible to calculate aggregates about general
statistics of the recommendations, such as view count and video duration, and the percentage of 
conspiracy videos present amongst them. This lead to four groups with fifteen entries of different 
statistics (one for each video watched). In order to find out whether any of the differences between the 
four groups were significant at any point, a number of ANOVAs were performed. 

\linenumbers

\subsection{Leaving the filter bubble}

\subsection{Machine learning}
\subsubsection{Data gathering}
To answer the research question, it is necessary to determine which YouTube videos can be considered
conspiracy videos. Considering the large amount of videos getting recommended, determining each video
manually is simply not possible. There are two possible ways to solve this problem. Firstly, there is a
dataset which contains nearly 7000 YouTube channels that have been manually labeled based on their
political view - almost 3000 of which were labeled as conspiracy channels \citep{ledwich2019algorithmic};
whenever a video is made by one such channel, it can be considered a conspiracy video. However, due to
the enormous amount of existing YouTube channels, the odds of a video being uploaded by a channel that
is not present in this dataset are very large. For those videos, a supervised machine learning
classifier was used. To optimize performance, five different classifiers have been trained and compared:
k-nearest neighbors, support-vector machine, neural network, logistic regression, and ridge regression. 

In order to train these machine learning algorithms, a training dataset was created. To get a labeled
dataset of conspiracy and non-conspiracy videos, use was made of the aforementioned channel dataset made
by \citet{ledwich2019algorithmic}. For each channel in that dataset, the title, description, and
transcript of the ten most recently uploaded videos were downloaded using YouTube's API. Videos uploaded
by a conspiracy channel were then labeled as conspiracy videos, and videos uploaded by a channel from a
different category were labeled as normal videos. Additionally, the channel description and channel
keywords (which are used for targeted advertising on YouTube) were added to each video. The final
dataset contains 65.683 unique YouTube videos, 22.156 of which are considered as conspiracy videos. 

\subsubsection{Data cleaning}
However, this dataset was not yet suitable for machine learning, as the data was still messy. Therefore,
multiple steps were taken in order to clean the data. Firstly, the two classes (conspiracy and
non-conspiracy) were balanced, so that the classifier would not develop a bias for non-conspiracy
videos. Rather than opting for balancing the two classes through the use of class-weights (a technique
where weights are attributed to classes, thereby telling the classifier that getting a prediction
correct for a certain, underrepresented class is more important), the choice
was made to under-sample the data in order to equalize both classes (both containing 22.156 videos, for
a total of 44.312 videos) \citep{lemaitre2017imbalanced, sun2006boosting}. As there was plenty of data
in the dataset, under-sampling was more convenient than implementing class-weights. After both classes
had been balanced, the text for each video had to be translated into English. Since the original dataset
by \citet{ledwich2019algorithmic} also contained channels by non-English speakers, these videos had to
be automatically translated. Then, a few common cleaning methods were applied: all text was converted to
lowercase, after which special characters, such as emojis were removed, whereafter stop words were
removed and all words were stemmed using the porter stemmer \citep{karaa2013new}. Finally, each video
was TF-IDF vectorized to allow the classifiers to function.

\subsubsection{Performance optimization}
After splitting the dataset into a training, test, and validation set, the hyperparameters of each
algorithm were tuned to get the optimal performance \citep{feurer2019hyperparameter}. Performance was
measured using four distinct metrics: the accuracy, which shows the share of correct predictions; the
recall, which shows what fraction of truly positive samples were correctly labeled as such; the
precision, which shows what part of the positive predictions were correct; and the F1-score, which is
the harmonic mean of the recall and precision \citep{sokolova2009systematic}. For each classifier,
different configurations of hyperparameters (such as the kernel and the penalty-parameter) were
systemically tested - each possible combination was tried. The classifiers were trained on the training set
and the optimal hyperparameters were determined based on the performance of the classifiers on the validation
set. By saving these performance measures for every configuration, for every classifier, the optimal
configuration for every classifier could be determined. Lastly, the classifiers were equipped with their
optimal hyperparameters and then tested for the final time on the test set. By comparing the performance
of every optimally configured classifier on the test set, the best-performing classifier could be chosen
\citep{reitermanova2010data}. 

Additionally, the added value of using a machine learning ensemble was measured. By having each
classifier make a prediction for all videos in the dataset, a new dataset was created, wherein the
features were the predictions made by the different classifiers. By using all possible combinations of
classifiers, and then having different neural networks use those features as input, a machine learning
ensemble was created. This ensemble was then optimized in a similar way to the classifiers individually.

\end{document}
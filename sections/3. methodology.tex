\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{images/}}}

\begin{document}

\section{Methodology}
Er zal in python een script worden geschreven dat YouTube-accounts video’s laten kijken en na elke video bijhoudt voor welk deel hun ‘aanbevolen’-pagina uit complotvideo’s bestaat. Het laten kijken van YouTube-video's door dit script, zal het mogelijk maken om de eerste deelvraag te beatwoorden (\textit{Bij welke kijkstrategie ontstaat er het snelst een complot-filterbubbel op YouTube?}). In totaal zullen er twintig YouTube-accounts worden aangemaakt, welke zullen worden onderverdeeld in vier kijkstrategieën:

\begin{itemize}
    \item Compleet willekeurige video’s (controlegroep);
    \item Willekeurige complotvideo’s uit een dataset;
    \item Eerst een willekeurige complotvideo en vervolgens telkens een aanbevolen video naast die video;
    \item Eerst een willekeurige complotvideo en vervolgens telkens een aanbevolen video op de YouTube-homepage van het account.
\end{itemize}

\noindent Elke kijkstrategie zal door vijf verschillende accounts worden uitgeprobeerd, en elk account zal 25 video’s kijken.   

Om het gedrag van echte gebruikers zo realistisch mogelijk te simuleren, zal de gemiddelde kijkduur van video’s normaal verdeeld worden, met een gemiddelde van 55\% en een standaardafwijking van 25\% \citep{park2016data}. Ook het klikgedrag van gebruikers zal zo accuraat mogelijk worden nagebootst. De kans dat een video op een bepaalde positie $k$ wordt aangeklikt (diens click-through rate: $CTR$), zal worden berekend met de volgende formule:

\begin{equation}
CTR(k; N, \alpha) = \frac{1/k^\alpha}{\sum_{n=1}^{N} (1/n^\alpha)}
\end{equation}

\noindent Waarin $N$ het totaal aantal video's in de lijst is en $\alpha$ de exponent-waarde die de distributie bepaalt. Op basis van de waardes gevonden door \citet{zhou2010impact}, zal de video op de eerste positie een klikkans van ongeveer 20.6\% krijgen, waarna de klikkans snel af zal lopen, tot 1.9\% bij de twintigste aanbeveling. 

Vervolgens zullen alle accounts compleet willekeurige video's gaan kijken. Dit wordt gedaan om deelvraag 2 (\textit{Hoe lang duurt het voor een YouTube-gebruiker om uit een filterbubbel te komen, wanneer deze zich erin bevindt?}) te beantwoorden. Er zal gekeken worden hoe lang het duurt voor de gebruikers in filterbubbels om een statistisch gelijke aanbevelingsdiversiteit te krijgen aan de gebruikers in de controlegroep. Zodra het aantal complotvideo's in de aanbevelingen niet meer significant verschilt met die van de controlegroep, zit de gebruiker niet meer in een filterbubbel.

Voor beide voorgaande onderzoeksvragen is het van belang om te kunnen bepalen of video's tellen als complotvideo's of niet. Om dit te doen, zijn er twee mogelijkheden. Als eerst is er een dataset met daarin 3000 YouTube-kanalen die door mensen zijn gelabeld als complotkanalen \citep{ledwich2019algorithmic}, dus een video die geüpload is door een dergelijk kanaal zal tellen als complotvideo. Daarnaast, voor video’s die buiten deze kanalen vallen, zal een ensemble van machine learning classifiers gebruikt worden, om zo te bepalen welke video’s wel en niet als complotvideo’s gezien kunnen worden. Dit leidt tot de derde onderzoeksvraag (\textit{Welk type classifier werkt het beste om complotvideo’s op YouTube te labelen?}). Om deze vraag te beantwoorden, en daarmee ook de prestatie van de classifier te optimaliseren, zal er gekeken worden welk type machine learning classifier het beste is in het identificeren van complotvideo’s. De volgende algoritmes zullen worden getest: k-nearest neighbors, support-vector machine, neural network, logistic regression, en ridge regression. Al deze algoritmes zullen worden getraind op een dataset bestaande uit 40.000 YouTube-video’s, welke zijn gelabeld als wel/geen complotvideo’s. Voor elke video zal de titel, beschrijving en ondertiteling, plus de beschrijving en keywords van het bijbehorende YouTube-kanaal beschikbaar zijn. De dataset zal schoongemaakt worden op verschillende manieren: de twee klassen zullen worden gebalanceerd, niet-Engelse teksten zullen worden vertaald, woorden zullen gestemd worden en stopwoorden zullen verwijderd worden. Vervolgens zal de tekst gevectoriseerd worden op basis van TF-IDF waardes. Door middel van een train/test/validation-split van de data zal het mogelijk zijn om voor elk algoritme de hyperparameters te optimaliseren, om zodanig de best mogelijke prestatie te verkrijgen. Deze prestatie zal geëvalueerd worden op basis van vier verschillende waardes: de accuracy, die het aandeel correcte voorspelling uitdrukt; de recall, die aangeeft welk deel van de daadwerkelijk-positieve waardes ook als positief is voorspeld; de precision, die aangeeft welk deel van de positief-voorspelde waardes daadwerkelijk positief is; en de F1-score, die het harmonisch gemiddelde van de recall en precision uitdrukt \citep{sokolova2009systematic}. Door een zo hoog mogelijke, maar gebalanceerde, waarde te vinden voor al deze waardes, zal het mogelijk zijn om de optimale hyperparameters te kiezen en vervolgens te bepalen welke classifier de beste prestatie levert.

Als laatste zal er gekeken worden naar de toegevoegde waarde van het gebruik van een machine learning ensemble. Aangezien er twee mogelijke labels zijn voor de data, zal dit ensemble bestaan uit twee verschillende classifiers \citep{bonab2016theoretical, bonab2017less}. Voor dit ensemble zullen wederom de hyperparameters worden getuned (waaronder welke twee classifiers gebruikt zullen worden), om op die manier te onderzoeken of een ensemble beter presteert dan losse classifiers op zichzelf. Het resultaat hiervan zal antwoord bieden op de eerste deelvraag.

\end{document}
\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{images/}}}

\begin{document}

\section{Introduction}
YouTube attracts an average of 34.6 billion page views per month, making it the world's largest
video-sharing website and the second largest website on the entire internet \citep{neufeld_2021}. The
overwhelming majority of those page views come from users watching videos, 70\% of which are recommended
to users by YouTube's algorithm \citep{cooper_2020}. All types of content get produced and consumed on
the website. However, conspiracy content has been booming on YouTube \citep{donzelli2018misinformation}.
Alt-right (or far-right) and conspiracy channels are starting to grow their audiences, which could have
many negative consequences for society at large. For example, the number of people who are distrustful
of science is increasing, a development in which conspiracy content on YouTube plays a role. Whenever
this increased distrust relates to important topics, such as believing in the efficacy of vaccines, it
can create genuine dangers to the public. As it turns out, more than half of the American population has
doubts about - or is definitely against - taking the COVID-19 vaccine \citep{rosenbaum2021escaping}. 

To better understand how YouTube's algorithm (and recommender algorithms in general) allow(s) conspiracy
content to thrive, this research will investigate how quickly the algorithm develops a preference for 
conspiracy video; in other words: how many videos a user needs to watch before they get sent 
\textit{down the rabbit hole}. 

This research takes inspiration from the Dutch television program \textit{Zondag met Lubach}, where a
similar idea was executed on a smaller scale \citep{lubach_2020}. Lubach's experiment consisted of 
creating a new YouTube account on a never-used laptop, after which the account was used to watch a few 
(recommended) conspiracy videos, to see how this would affect the YouTube homepage of the account. After
a mere three videos, the homepage of the account was filled to the brim with conspiracy content, mostly 
having to do with the coronavirus. The extremely interesting results that came from this 
micro-experiment formed the motivation to research this phenomenon more extensively. 

\subsection{Research question}
To research this subject, the following research question has been formulated:
\textit{What is the impact of different watch strategies on the number of conspiracy videos that have to
be watched until a user's YouTube-recommendations start preferring conspiracy content?} In this scenario,
'preferring' will be defined as the situation in which the amount of conspiracy videos present in the
recommendations is significantly higher than that of the baseline.

To assist in answering the research question, the following sub-questions will be answered:
\begin{itemize}
    \item How do different watch strategies on YouTube influence the type of content that is recommended to a user?
    \item How long does it take for a YouTube recommendations to stop preferring conspiracy videos, once they have started doing so?
    \item What type of classifier performs the best when it comes to labeling conspiracy videos on YouTube?
\end{itemize}

\end{document}
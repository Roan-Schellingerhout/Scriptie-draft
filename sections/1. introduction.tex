\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{images/}}}

\begin{document}

\section{Introduction}
YouTube attracts an average of 34.6 billion page views per month, making it the world's largest
video-sharing website and the second largest website on the entire internet \citep{neufeld_2021}. The
overwhelming majority of those page views come from users watching videos, 70\% of which are recommended
to users by YouTube's algorithm \citep{cooper_2020}. All types of content get produced and consumed on
the website. However, conspiracy content has been booming on YouTube \citep{donzelli2018misinformation}.
Alt-right (or far-right) and conspiracy channels are starting to grow their audiences, which could have
many negative consequences for society at large. For example, the number of people who are distrustful
of science is increasing, a development in which conspiracy content on YouTube plays a role. Whenever
this increased distrust relates to important topics, such as believing in the efficacy of vaccines, it
can create genuine dangers to the public. As it turns out, more than half of the American population has
doubts about - or is definitely against - taking the COVID-19 vaccine \citep{rosenbaum2021escaping}. 

To better understand how YouTube's algorithm (and recommender algorithms in general) allow(s) conspiracy
content to thrive, this research will investigate how quickly the algorithm develops a preference for 
conspiracy video; in other words: how many videos a user needs to watch before they get sent 
\textit{down the rabbit hole}. 

This research is based on the assumption that YouTube's recommender system is susceptible to the creation of 
filter bubbles. This concept, coined by \citet{pariser2011filter}, has been studied in-depth on many social 
media websites, YouTube included. While results vary slightly, the common finding is that YouTube 
recommendations do indeed lead to filter bubbles and that extremist and conspiracy content is more likely to 
do so \citep{bryant2020youtube, o2013extreme, ledwich2020algorithmic}. This effect can lead to the 
radicalization of impressionable users, with deleterious consequences. However, an important factor herein is
how quickly a user's recommendations turn into a bubble. If the user has enough time to be exposed to other 
types of content, they might stray away from the more extreme, preventing them from adopting a potentially 
harmful view \citep{bozdag2015breaking}. That is why this research looks at how \textit{quickly} a user could
end up in a filter bubble on YouTube. To do so, brand-new accounts will be made to watch YouTube videos 
according to different watch strategies; after each video watched, the recommendations of the user will be 
labeled as being either conspiracy content or regular content by a machine learning classifier, to determine 
whether or not the user is in a bubble. 

\subsection{Research question}
For this research the following research question has been formulated:
\textit{What is the impact of different watch strategies on the number of conspiracy videos that have to
be watched until a user's YouTube recommendations start preferring conspiracy content?} In this scenario,
'preferring' will be defined as the situation in which the amount of conspiracy videos present in the
recommendations is significantly higher than that of the baseline.

To assist in answering the research question, the following sub-questions will be answered:
\begin{itemize}
    \item How do different watch strategies on YouTube influence the type of conspiracy content that is recommended to a user?
    \item How long does it take for YouTube recommendations to stop preferring conspiracy videos, once they have started doing so?
    \item What type of classifier is suitable for labeling conspiracy videos on YouTube?
\end{itemize}

\end{document}
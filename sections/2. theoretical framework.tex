\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{images/}}}

\begin{document}

\section{Theoretical Framework}
\subsection{Filter bubbles on social media}
Whenever the user of a website finds themselves in their own information universe, in which the content
and recommendations play into the user's preexisting opinions and believes, they are in a filter bubble
\citep{pariser2011filter}. Users are by themselves in such bubbles and each bubble is unique. Different
bubbles can have overlap, but each bubble is precisely tuned to an individual. In traditional media, a
user makes a conscious \textit{choice} what types of opinions they want to hear, for example by choosing
to watch a broadcaster with a specific political opinion. Online this decision is implicit: based on the
user's behavior, their content is filtered automatically by an algorithm, without explicit consent. 

\subsection{Filter bubbles on YouTube}
Previous research has found that YouTube's recommendation algorithm runs the risk of creating filter
bubbles. \citet{roth2020tubes} came to this conclusion after they analysed YouTube recommendations based
on content. YouTube has two distinct types of recommendations: recommendations based on the user's
viewing behavior and recommendations based on the content of the current video a user is watching. In
their research, Roth et al. focused predominantly on recommendations based on content. They found that
such recommendations could quickly lead to a decrease in information diversity (thus, filter bubbles)
and that this decrease happened sooner for videos with a lot of views; the more views a video had, the
less diverse its related recommendations. They speculate that this can be explained by the fact that
YouTube tends to store more information about videos with a high view count, allowing the algorithm to
give better recommendations for such videos. They also predict that, whenever the algorithm has more
information about a user to its disposal, it can combine said information with the information it has
about a certain video, which could lead to an even stronger limitation of recommendations. According to
\citet{ledwich2020algorithmic}, a user's viewing behavior is responsible for approximately 70\% of their
recommendations; this behavior could therefore play a big role in the creation of filter bubbles on
YouTube. 

Once a YouTube user has entered a filter bubble, it can be difficult to escape it. The most common way to 
help users get out of a filter bubble is by exposing them to content covering viewpoints other than their own
\citep{bozdag2015breaking}. However, for YouTube, this could form an issue. YouTube makes its money by 
displaying advertisements to a user. The longer a user stays on the website, the more profit YouTube can 
make. As a result, YouTube's algorithm prefers recommending videos that are likely to generate a lot of watch
time \citep{maack_2019}. As it turns out, controversial content (such as conspiracies) tends to have a higher
audience retention: people keep watching controversial content for longer \citep{birch2019white}. Whenever 
content is surprising (which conspiracy theories often are), it is more likely to capture and keep a user's 
attention. Thus, by showing the user more diverse content, the algorithm would actively hinder its own goal.
Because of this design, filter bubbles are commonplace on YouTube. 

\subsection{Conspiracy content on YouTube}
YouTube has limited rules with regards to the spread of conspiracy videos \citep{youtube_2021}. As long
as the content does not directly incite violence or endangers the public health (e.g. misinformation
about the COVID-19 virus), objectively incorrect ideas are allowed to be shared on YouTube. As a result,
YouTube is a home to multiple conspiracy communities. Conspiracy theories such as 'the earth is flat and
the government is hiding it from us', 'the world will end soon and only followers of this specific
religion will be spared', and 'the world is ruled by cannibalistic, satanic pedophiles' (better known as
QAnon) gather millions of views on the platform \citep{paolillo2018flat, miller2021characterizing}.
Though such videos could be considered harmful to society, they are not suppressed by YouTube. Whenever
a user shows interest in this type of content, they will be recommended similar videos, even when
YouTube is aware of their harmful nature \citep{ledwich2020algorithmic, maack_2019}.

\subsection{The YouTube algorithm}
The YouTube algorithm tries to recommend videos based on the expected watch time they will generate,
rather than the probability of a user clicking on them \citep{covington2016deep}. This decision was made
in order to decrease the likelihood of misleading videos (also known as \textit{clickbait}) being
recommended. However, gathering feedback about videos through their watch time can cause a lot of noise,
making it difficult to measure user satisfaction. As it turns out, even when a users enjoy a certain
video, they are unlikely to watch it completely. On average, users watch around 50-60\% of a video
before they switch it off \citep{park2016data}. But, videos that are well-structured, or especially
interesting, can improve this percentage up till 70-80\%, where nearly half of the viewers actually
finish the video in its entirety \citep{lang_2018}. After a video has been watched, there is a 41.6\%
chance that the user decides to watch a recommended video. Which recommendation the user will choose,
follows a Zipf-distribution ($\alpha = 0.78$) with regards to the position of the video in the list of
recommendations \citep{zhou2010impact}. 

All in all, previous research has found that YouTube's algorithm is sensitive to filter bubbles and that
it has a tendency to recommend conspiracy content. It is also speculated that the algorithm makes
decisions based on the user's viewing behavior, which it combines with the content of videos. In order
to keep the user on the website as long as possible, which is profitable for YouTube, the algorithm
prefers recommending videos that it suspects the user will watch for a longer period of time, even when
they may contain harmful content. Based on this information, further research can be done on the
origination of filter bubbles and the spread of conspiracy content on YouTube. For example, little is
known about how quickly a user's recommendations adapt to a user's behavior, even though this is a
critical aspect when it comes to the creation of so-called \textit{rabbit holes}. Furthermore, no
research has been done into the way different types of videos (recommendations in different locations,
random videos, etc.) influence YouTube's algorithm. For example, whenever a user primarily watches
recommended videos, the algorithm could see this as implicit positive feedback, which could cause a
snowball-effect. 
\end{document}
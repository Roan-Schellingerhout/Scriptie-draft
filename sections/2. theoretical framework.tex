\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{images/}}}

\begin{document}

\section{Theoretical Framework}
Wanneer een gebruiker van een website zich bevindt in een eigen informatie-‘universum’, waarin de content en aanbevelingen inspelen op diens bestaande meningen en overtuigingen, zit deze in een filterbubbel \citep{pariser2011filter}. Gebruikers zijn alleen in zulke bubbels; iedereen heeft een eigen, unieke bubbel. Er kan overlap zijn tussen de bubbels van verschillende personen, maar elke bubbel is precies aangepast op het individu. In traditionele media kan een persoon ervoor \textit{kiezen} welk type meningen deze wil aanhoren, door bijvoorbeeld naar een specifieke nieuwszender te kijken. Online is deze keuze niet expliciet, op basis van het gedrag van de gebruiker wordt deze automatisch, zonder toestemming, een bepaald filter voorgelegd. Uit voorgaand onderzoek is gebleken dat het YouTube-algoritme dat verantwoordelijk is voor het geven van aanbevelingen gevoelig is voor het ontstaan van dergelijke filterbubbels. \citet{roth2020tubes} zijn tot deze conclusie gekomen nadat zij YouTube-aanbevelingen op basis van content hebben geanalyseerd. Op YouTube zijn er twee soorten aanbevelingen: aanbevelingen gebaseerd op het kijkgedrag van de gebruiker en aanbevelingen gebaseerd op de content van de huidige video. In hun onderzoek hebben Roth et al. de focus gelegd op aanbevelingen op basis van content. Hieruit bleek dat dergelijke aanbevelingen snel konden leiden tot lage diversiteit (oftewel: filterbubbels) en dat deze bubbels sneller optraden bij video’s met meer weergaven; des te meer weergaven een video had, des te minder divers de gerelateerde aanbevelingen. Zij speculeren dat dit verklaard kan worden door het feit dat YouTube meer informatie opslaat over video’s met veel weergaven, waardoor het algoritme betere aanbevelingen kan geven. Ook voorspellen zij dat, naarmate het algoritme meer informatie verkrijgt over de gebruiker, het deze informatie kan combineren met de informatie over video’s, wat zou leiden tot een nog sterkere beperking van de aanbevelingen. Volgens \citet{ledwich2019algorithmic} is het kijkgedrag van de gebruiker verantwoordelijk voor ongeveer 70\% van de aanbevelingen; kijkgedrag zou daarom veel invloed kunnen hebben op het ontstaan van filterbubbels op YouTube.  

YouTube heeft beperkte regels tegen het verspreiden van complottheorieën (YouTube, 2021). Zolang de content niet direct aanzet tot geweld of de volksgezondheid in gevaar brengt (denk hierbij aan misinformatie over het COVID-19-virus), mogen ook objectief onjuiste ideeën worden verspreid via YouTube. Dit zorgt ervoor dat er op YouTube meerdere complotgemeenschappen huisvesten. Complottheorieën als ‘de aarde is plat en de overheid probeert dat te verstoppen’, ‘de wereld zal binnenkort vergaan en alleen aanhangers van een bepaalde religie zullen overleven’, en ‘de wereld wordt geregeerd door kannibalistische, satanische pedofielen’ (beter bekend als QAnon), worden op de website door miljoenen mensen bekeken \citep{paolillo2018flat, miller2021characterizing}. Hoewel dergelijke video’s schadelijk kunnen zijn voor de maatschappij, worden deze door YouTube niet onderdrukt. Als een gebruiker interesse toont in zulke video’s, zal de gebruiker soortgelijke video’s aanbevolen krijgen, ook als YouTube ervan op de hoogte is dat het mogelijk schadelijke content is \citep{ledwich2019algorithmic}.

Het YouTube-algoritme probeert aanbevelingen te doen op basis van de verwachtte kijktijd (watch time) en niet op basis van de kans dat een gebruiker op een video klikt \citep{covington2016deep}. Dit is gedaan om misleidende video’s (beter bekend als ‘clickbait’) een lagere kans te geven om aanbevolen te worden. Maar, het verkrijgen van feedback over een video op basis van kijktijd heeft te kampen met veel ruis, waardoor het lastig is om tevredenheid te meten. Zo blijkt dat, ook wanneer een persoon een video interessant vindt, deze de video vaak niet helemaal afkijkt. Gemiddeld kijken personen ongeveer 50-60\% van een video voordat zij hiervan wegklikken (Park et al., 2016). Echter, video’s die goed gestructureerd, of erg interessant zijn, kunnen dit percentage omhoog halen tot 70-80\%, waarin ongeveer de helft van de kijkers de video zelfs volledig afkijkt (Lang, 2018). Nadat een video is afgekeken, is er een 41.6\% kans dat de gebruiker een aanbevolen video zal bekijken. Welke video dit wordt, volgt een Zipf’s verdeling ($\alpha = 0.78$) op basis van de positie in de lijst van aanbevelingen \citep{zhou2010impact}. 

In voorgaand onderzoek is er dus vernomen dat filterbubbels zich voordoen op YouTube en dat het algoritme geneigd is om complot-content aan te bevelen. Ook wordt er gespeculeerd dat het algoritme beslissingen maakt op basis van het kijkgedrag van gebruikers en dit koppelt aan de content van video's. Om de gebruiker zo lang mogelijk op de website te houden, wat winstgevend is voor YouTube, probeert het algoritme video's aan te bevelen die de gebruiker waarschijnlijk lang zal kijken, welke soms schadelijke content bevatten. Op basis van deze informatie kan er dus verder onderzoek worden gedaan naar het ontstaan van filterbubbels en het verspreiden van complot-content op YouTube. Zo is er nog weinig bekend over hoe snel de aanbevelingen van een gebruiker zich aanpassen, terwijl dit erg belangrijk kan zijn in het ontstaan van zogeheten 'rabbit holes'. Ook is er nog geen onderzoek gedaan naar hoe het soort video invloed heeft op het algoritme. Wanneer een gebruiker veel aanbevelingen kijkt, zou dit mogelijk gezien kunnen worden als impliciete positieve feedback, waardoor er een sneeuwbaleffect kan ontstaan. 

\end{document}
\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{images/}}}

\begin{document}

\section{Conclusion}
The goal of this research was to find out how different watch strategies affect the speed with which a user's YouTube 
homepage recommendations start preferring conspiracy content. Through the use of twenty brand-new YouTube accounts, an 
experiment was conducted in which different watch strategies were used to watch conspiracy content. The watch strategies
were set-up to have an increasing level of personalization, in order to find a connection between the level of
personalization and the speed with which a filter bubbles emerges. It was established that users who watch more
personalized content tend to not only have their recommendations prefer conspiracy content more quickly, but also have
this preference be stronger. This is in line with the original hypothesis. Additionally, it was found that, on YouTube, it
is significantly more difficult to escape a filter bubble than to end up in one, regardless of the level of
personalization (although increased personalization does lead to the filter bubbles remaining \textit{stronger} for
longer).

While previous research on the topic has already been done, this research made use of indirect or less-personalized
methods to gather information about YouTube's algorithm. Furthermore, the results found by previous research have been
contradictory to a certain degree. By therefore setting up the experiments in such a way that they accurately reflect real
user behavior, new insights were gained into the way in which the YouTube algorithm is susceptible to filter bubbles.
While this research has been relatively small-scale, the results are decisive: the YouTube algorithm is vulnerable to filter bubbles, even when it comes to harmful content.  

\end{document}